{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import spacy\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import xgboost as xgb\n",
    "\n",
    "import nltk\n",
    "\n",
    "from time import time\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation, PCA\n",
    "from sklearn.metrics import f1_score, classification_report, multilabel_confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_predict, cross_validate, cross_val_score, learning_curve\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "# Ignorance de certains avertissements\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fonctions utiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(texte):\n",
    "    \n",
    "    char_gardes = r'[^a-z#+.\\s]'\n",
    "    return re.sub(char_gardes, '', texte)  \n",
    "\n",
    "def remove_sw(texte, return_verbs, return_token):\n",
    "    \n",
    "    doc = nlp(texte)\n",
    "    \n",
    "    if return_verbs :\n",
    "    \n",
    "        tokens = [token for token in doc if \\\n",
    "                  token.is_stop == False and token.is_punct == False\\\n",
    "                  and token.is_space == False\n",
    "                 ]\n",
    "    \n",
    "    else :\n",
    "        \n",
    "        tokens = [token for token in doc if \\\n",
    "                  token.is_stop == False and token.is_punct == False\\\n",
    "                  and token.pos_ != \"VERB\"\\\n",
    "                  and token.is_space == False\n",
    "                 ]\n",
    "    \n",
    "    if return_token :\n",
    "        \n",
    "        return tokens\n",
    "    \n",
    "    txt = \"\"\n",
    "    \n",
    "    for t in tokens :\n",
    "        \n",
    "        txt = txt + \" \" + t.text\n",
    "        txt = txt.lstrip()\n",
    "        \n",
    "    return txt\n",
    "\n",
    "\n",
    "def pipe_clsw(texte, return_verbs = True, return_token = True):\n",
    "    \"\"\"\n",
    "    Nettoye et enlève les SW d'un text/str et retourne une liste de tokens.\n",
    "    Par défaut : \n",
    "    - On garde les verbe. Si False on les élimine du résultat.\n",
    "    - On retourne une liste de token. Si False on retourne une str des tokens.\n",
    "    \"\"\"\n",
    "    \n",
    "    res = remove_sw(clean_text(texte), return_verbs, return_token)\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Approche 100% SUPERVISEE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-) Approche PCA directe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On repart de nos données vectorisées - normalisée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_train_vect = pickle.load(open(\"Data/feat_train_vect.pickle\", \"rb\"))\n",
    "feat_test_vect = pickle.load(open(\"Data/feat_test_vect.pickle\", \"rb\"))\n",
    "\n",
    "y_train = pickle.load(open(\"Data/y_train.pickle\", \"rb\"))\n",
    "y_test = pickle.load(open(\"Data/y_test.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 4789)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_train_vect.toarray().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Réductions dimensionnelles**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "std = StandardScaler().fit(feat_train_vect.toarray())\n",
    "feat_train_scaled = std.transform(feat_train_vect.toarray())\n",
    "feat_test_scaled = std.transform(feat_test_vect.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1-) PCA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pour n = 5, la PCA explique 0.015277060867347151 % de la variance des données\n",
      "Pour n = 10, la PCA explique 0.024608497457983547 % de la variance des données\n",
      "Pour n = 100, la PCA explique 0.10618675848159564 % de la variance des données\n",
      "Pour n = 500, la PCA explique 0.2975371731613061 % de la variance des données\n",
      "Pour n = 1000, la PCA explique 0.4690885327636175 % de la variance des données\n"
     ]
    }
   ],
   "source": [
    "for n in [5, 10, 100, 500, 1000]:\n",
    "    pca = PCA(n_components = n)\n",
    "    pca.fit(feat_train_scaled)\n",
    "    print(f\"Pour n = {n}, la PCA explique {pca.explained_variance_ratio_.sum()} % de la variance des données\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une réduction en **1000** composants principaux n'explique que **47%** de la variance des données.<br>Manifestement la PCA ne va pas nous permettre de réduire suffisamment le nombre de dimension de nos données. Mais on va quand même faire un essai pour **n = 1000**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_1000 = PCA(n_components = 1000).fit(feat_train_scaled)\n",
    "X_train = pca_1000.transform(feat_train_scaled)\n",
    "X_test = pca_1000.transform(feat_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 1000)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bo_tune_m_clas3(max_depth, gamma, n_estimators ,learning_rate, eta):\n",
    "    params = {'max_depth': int(max_depth),\n",
    "              'gamma': gamma,\n",
    "              'n_estimators': int(n_estimators),\n",
    "              'learning_rate': learning_rate,\n",
    "              'subsample': 0.8,\n",
    "              'eta': eta, \n",
    "              'verbosity': 0, \n",
    "              'objective': \"reg:logistic\",\n",
    "              'random_state' : 47#, 'eval_metric': 'f1'\n",
    "             }\n",
    "    \n",
    "    # création du model qu'on va utiliser\n",
    "    xgb_model = xgb.XGBClassifier(**params)\n",
    "    # création du modèle multilabels\n",
    "    m_clas = BinaryRelevance(classifier = xgb_model)\n",
    "    \n",
    "    # Cross validating with the specified parameters in 5 folds and 50 iterations\n",
    "    cv_result = cross_validate(m_clas, X_train, y_train, scoring = \"f1_weighted\")\n",
    "    #Return the negative the error\n",
    "    #print(cv_result[\"test_score\"].mean())\n",
    "    return cv_result[\"test_score\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_clas_bo3 = BayesianOptimization(bo_tune_m_clas3, {'max_depth': (3, 10),\n",
    "                                                  'gamma': (0, 1),\n",
    "                                                  'eta' : (0.01, 0.1),\n",
    "                                                  'learning_rate':(0, 1),\n",
    "                                                  'n_estimators':(50, 250)\n",
    "                                                 })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   |    eta    |   gamma   | learni... | max_depth | n_esti... |\n",
      "-------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32me:\\anaconda3\\envs\\OpClass\\lib\\site-packages\\bayes_opt\\target_space.py\u001b[0m in \u001b[0;36mprobe\u001b[1;34m(self, params)\u001b[0m\n\u001b[0;32m    190\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 191\u001b[1;33m             \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0m_hashable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    192\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: (0.033523752894924895, 0.5006776782584547, 0.42937492649307474, 9.5032264913649, 227.92242676727906)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-bb18f4bc38ad>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mm_clas_bo3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaximize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minit_points\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ucb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"L'optimization a mis : {time() - start:.0f} secondes.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\anaconda3\\envs\\OpClass\\lib\\site-packages\\bayes_opt\\bayesian_optimization.py\u001b[0m in \u001b[0;36mmaximize\u001b[1;34m(self, init_points, n_iter, acq, kappa, xi, **gp_params)\u001b[0m\n\u001b[0;32m    172\u001b[0m                 \u001b[0miteration\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 174\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprobe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_probe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlazy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    175\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    176\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOPTIMIZATION_END\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\anaconda3\\envs\\OpClass\\lib\\site-packages\\bayes_opt\\bayesian_optimization.py\u001b[0m in \u001b[0;36mprobe\u001b[1;34m(self, params, lazy)\u001b[0m\n\u001b[0;32m    110\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_queue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 112\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprobe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    113\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOPTIMIZATION_STEP\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\anaconda3\\envs\\OpClass\\lib\\site-packages\\bayes_opt\\target_space.py\u001b[0m in \u001b[0;36mprobe\u001b[1;34m(self, params)\u001b[0m\n\u001b[0;32m    192\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    193\u001b[0m             \u001b[0mparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_keys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 194\u001b[1;33m             \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    195\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-45-07c64ff9988a>\u001b[0m in \u001b[0;36mbo_tune_m_clas3\u001b[1;34m(max_depth, gamma, n_estimators, learning_rate, eta)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;31m# Cross validating with the specified parameters in 5 folds and 50 iterations\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m     \u001b[0mcv_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_validate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm_clas\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"f1_weighted\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m     \u001b[1;31m#Return the negative the error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;31m#print(cv_result[\"test_score\"].mean())\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\anaconda3\\envs\\OpClass\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\anaconda3\\envs\\OpClass\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    240\u001b[0m     parallel = Parallel(n_jobs=n_jobs, verbose=verbose,\n\u001b[0;32m    241\u001b[0m                         pre_dispatch=pre_dispatch)\n\u001b[1;32m--> 242\u001b[1;33m     scores = parallel(\n\u001b[0m\u001b[0;32m    243\u001b[0m         delayed(_fit_and_score)(\n\u001b[0;32m    244\u001b[0m             \u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscorers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\anaconda3\\envs\\OpClass\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1039\u001b[0m             \u001b[1;31m# remaining jobs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1040\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1041\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1042\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1043\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\anaconda3\\envs\\OpClass\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    857\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 859\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    860\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\anaconda3\\envs\\OpClass\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    775\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    776\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 777\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    778\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\anaconda3\\envs\\OpClass\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\anaconda3\\envs\\OpClass\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    570\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 572\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\anaconda3\\envs\\OpClass\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[1;31m# change the default number of processes to -1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m             return [func(*args, **kwargs)\n\u001b[0m\u001b[0;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\anaconda3\\envs\\OpClass\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[1;31m# change the default number of processes to -1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m             return [func(*args, **kwargs)\n\u001b[0m\u001b[0;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\anaconda3\\envs\\OpClass\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    529\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 531\u001b[1;33m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    532\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    533\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\anaconda3\\envs\\OpClass\\lib\\site-packages\\skmultilearn\\problem_transform\\br.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    159\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_subset\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0my_subset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0my_subset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m                 \u001b[0my_subset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_subset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 161\u001b[1;33m             classifier.fit(self._ensure_input_format(\n\u001b[0m\u001b[0;32m    162\u001b[0m                 X), self._ensure_output_format(y_subset))\n\u001b[0;32m    163\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassifiers_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\anaconda3\\envs\\OpClass\\lib\\site-packages\\xgboost\\sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, callbacks)\u001b[0m\n\u001b[0;32m    826\u001b[0m                                 missing=self.missing, nthread=self.n_jobs)\n\u001b[0;32m    827\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 828\u001b[1;33m         self._Booster = train(xgb_options, train_dmatrix,\n\u001b[0m\u001b[0;32m    829\u001b[0m                               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_num_boosting_rounds\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m                               \u001b[0mevals\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\anaconda3\\envs\\OpClass\\lib\\site-packages\\xgboost\\training.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks)\u001b[0m\n\u001b[0;32m    206\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecord_evaluation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m     return _train_internal(params, dtrain,\n\u001b[0m\u001b[0;32m    209\u001b[0m                            \u001b[0mnum_boost_round\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_boost_round\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\anaconda3\\envs\\OpClass\\lib\\site-packages\\xgboost\\training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[1;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m             \u001b[0mbst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m             \u001b[0mbst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m             \u001b[0mversion\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\anaconda3\\envs\\OpClass\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[0;32m   1157\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1158\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1159\u001b[1;33m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,\n\u001b[0m\u001b[0;32m   1160\u001b[0m                                                     \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1161\u001b[0m                                                     dtrain.handle))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "m_clas_bo3.maximize(n_iter=10, init_points=5, acq='ucb')\n",
    "print(f\"L'optimization a mis : {time() - start:.0f} secondes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trop long on abandonne..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Word Embedding Spacy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat = pickle.load(open(\"Data/X.pickle\", \"rb\"))\n",
    "targ = pickle.load(open(\"Data/y.pickle\", \"rb\"))\n",
    "\n",
    "feat_train = feat[:20000]\n",
    "feat_test = feat[20000:]\n",
    "\n",
    "targ_train = targ[:20000]\n",
    "targ_test = targ[20000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ce travail de nettoyage a pris 422 secondes.\n"
     ]
    }
   ],
   "source": [
    "# on garde les verbes, mais on veut le résultat en \"string\"\n",
    "now = time()\n",
    "feat_train_clean = [pipe_clsw(t, True, False) for t in feat_train]\n",
    "feat_test_clean = [pipe_clsw(t, True, False) for t in feat_test]\n",
    "print(f\"Ce travail de nettoyage a pris {time() - now:.0f} secondes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'spring boot controller content negotiation simple rest controller written springboot application sure implement content negotiation return json xml based contenttype parameter request header explain wrong controller method json calling method specify contenttype applicationxml textxml implement methods different mapping different content type able xml xml work specify mediatypes single method like provided example like message endpoint receive xml contenttype request set applicationxmljson contenttype applicationjson help appreciated editi updated controller accept media types'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_train_clean[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_out = open(\"Data/feat_train_clean.pickle\", \"wb\")\n",
    "pickle.dump(feat_train_clean, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "pickle_out = open(\"Data/feat_test_clean.pickle\", \"wb\")\n",
    "pickle.dump(feat_test_clean, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_train_clean = pickle.load(open(\"Data/feat_train_clean.pickle\", \"rb\"))\n",
    "feat_test_clean = pickle.load(open(\"Data/feat_test_clean.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chargement du modèle\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cette transformation Word-Embedding avec Spacy a pris 291 secondes.\n"
     ]
    }
   ],
   "source": [
    "# transformation du corpus\n",
    "now = time()\n",
    "X_train_spacy = np.array([nlp(w).vector for w in feat_train_clean])\n",
    "X_test_spacy = np.array([nlp(w).vector for w in feat_test_clean])\n",
    "print(f\"Cette transformation Word-Embedding avec Spacy a pris {time() - now:.0f} secondes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_out = open(\"Data/X_train_spacy.pickle\", \"wb\")\n",
    "pickle.dump(X_train_spacy, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "pickle_out = open(\"Data/X_test_spacy.pickle\", \"wb\")\n",
    "pickle.dump(X_test_spacy, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_spacy = pickle.load(open(\"Data/X_train_spacy.pickle\", \"rb\"))\n",
    "X_test_spacy = pickle.load(open(\"Data/X_test_spacy.pickle\", \"rb\"))\n",
    "\n",
    "y_train = pickle.load(open(\"Data/y_train.pickle\", \"rb\"))\n",
    "y_test = pickle.load(open(\"Data/y_test.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5555, 300)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_spacy.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PCA**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "300 dimensions c'est encore beaucoup, nous allons réutiliser la **PCA** pour réduire autant que possible ce nombre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "std = StandardScaler().fit(X_train_spacy)\n",
    "X_train_spacy_scaled = std.transform(X_train_spacy)\n",
    "X_test_spacy_scaled = std.transform(X_test_spacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pour n = 5, la PCA explique 25.85 % de la variance des données\n",
      "Pour n = 10, la PCA explique 37.20 % de la variance des données\n",
      "Pour n = 50, la PCA explique 69.11 % de la variance des données\n",
      "Pour n = 100, la PCA explique 83.68 % de la variance des données\n",
      "Pour n = 150, la PCA explique 91.34 % de la variance des données\n",
      "Pour n = 200, la PCA explique 95.91 % de la variance des données\n",
      "Pour n = 250, la PCA explique 98.74 % de la variance des données\n",
      "Pour n = 300, la PCA explique 100.00 % de la variance des données\n"
     ]
    }
   ],
   "source": [
    "for n in [5, 10, 50, 100, 150, 200, 250, 300]:\n",
    "    pca = PCA(n_components = n)\n",
    "    pca.fit(X_train_spacy_scaled)\n",
    "    print(f\"Pour n = {n}, la PCA explique {pca.explained_variance_ratio_.sum()*100:.2f}\\\n",
    "    % de la variance des données\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essayons maintenant des modélisations en fonction de **n**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelize(n, X_train, y_train, X_test, y_test):\n",
    "    \n",
    "    start = time()\n",
    "    \n",
    "    pca = PCA(n_components = n)\n",
    "    pca.fit(X_train_spacy_scaled)\n",
    "    X_train_pca = pca.transform(X_train)\n",
    "    X_test_pca = pca.transform(X_test)\n",
    "    \n",
    "    xgb_model = xgb.XGBClassifier()\n",
    "    m_class = BinaryRelevance(classifier = xgb_model).fit(X_train_pca, y_train)\n",
    "    pred_train = m_class.predict(X_train_pca)\n",
    "    pred_test = m_class.predict(X_test_pca)\n",
    "    \n",
    "    score_train = f1_score(y_train, pred_train, average = \"weighted\")\n",
    "    score_test = f1_score(y_test, pred_test, average = \"weighted\")\n",
    "    \n",
    "    temps = (time() - start)\n",
    "    \n",
    "    return (score_train, score_test, temps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for n in [5, 10, 50, 100, 150, 200, 250, 300]:\n",
    "    \n",
    "    results.append(modelize(n, X_train_spacy_scaled, y_train, X_test_spacy_scaled, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.49258234078897284, 0.11604894114651894, 23.865284204483032),\n",
       " (0.7788768095214799, 0.20266677765162033, 59.51931405067444),\n",
       " (0.9872378668581084, 0.4545490488435735, 172.51770997047424),\n",
       " (0.9959518214265013, 0.4911362155057302, 198.9031629562378),\n",
       " (0.9979583756542356, 0.4983300010230284, 264.8279039859772),\n",
       " (0.9992428273383103, 0.490155068485608, 331.8358862400055),\n",
       " (0.9996602350529881, 0.4867507452259863, 400.6845922470093),\n",
       " (0.9997925033389263, 0.4821342713785624, 480.8770890235901)]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optimisation bayesiène pour N=150**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components = 150)\n",
    "pca.fit(X_train_spacy_scaled)\n",
    "X_train_spacy_pca = pca.transform(X_train_spacy_scaled)\n",
    "X_test_spacy_pca = pca.transform(X_test_spacy_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l'entrainement a pris 248 secondes\n"
     ]
    }
   ],
   "source": [
    "now = time()\n",
    "xgb_model = xgb.XGBClassifier()\n",
    "m_class_test = BinaryRelevance(classifier = xgb_model).fit(X_train_spacy_pca, y_train)\n",
    "print(f\"l'entrainement a pris {time() - now:.0f} secondes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4959637586670044"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = m_class_test.predict(X_test_spacy_pca)\n",
    "\n",
    "f1_score(y_test, \n",
    "         preds, \n",
    "         average = \"weighted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "248 secondes pour une modélisation... Pour un résultat bien meilleure que ce qu'on a eu avant. On va tenter une recherche de meilleurs hyper-paramètres avec la méthode BO, et **sans** validation croisée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bo_tune_m_clas3(max_depth, gamma, n_estimators ,learning_rate, eta):\n",
    "    params = {'max_depth': int(max_depth),\n",
    "              'gamma': gamma,\n",
    "              'n_estimators': int(n_estimators),\n",
    "              'learning_rate': learning_rate,\n",
    "              'subsample': 0.8,\n",
    "              'eta': eta, \n",
    "              'verbosity': 0, \n",
    "              'objective': \"reg:logistic\",\n",
    "              'random_state' : 47#, 'eval_metric': 'f1'\n",
    "             }\n",
    "    \n",
    "    # création du model qu'on va utiliser\n",
    "    xgb_model = xgb.XGBClassifier(**params)\n",
    "    # création du modèle multilabels\n",
    "    m_clas = BinaryRelevance(classifier = xgb_model)\n",
    "    \n",
    "    # Cross validating with the specified parameters in 5 folds and 50 iterations\n",
    "    # cv_result = cross_validate(m_clas, X_train, y_train, scoring = \"f1_weighted\")\n",
    "    #Return the negative the error\n",
    "    #print(cv_result[\"test_score\"].mean())\n",
    "    \n",
    "    m_clas.fit(X_train_spacy_pca, y_train)\n",
    "    preds = m_clas.predict(X_test_spacy_pca)\n",
    "\n",
    "    score = f1_score(y_test, preds, average = \"weighted\")\n",
    "    \n",
    "    \n",
    "    # return cv_result[\"test_score\"].mean()\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_clas_bo3 = BayesianOptimization(bo_tune_m_clas3, \n",
    "                                  {'max_depth': (3, 10),\n",
    "                                   'gamma': (0, 1),\n",
    "                                   'eta' : (0.01, 0.1),\n",
    "                                   'learning_rate':(0, 1),\n",
    "                                   'n_estimators':(50, 250)}, \n",
    "                                  random_state = 47)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   |    eta    |   gamma   | learni... | max_depth | n_esti... |\n",
      "-------------------------------------------------------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.5258  \u001b[0m | \u001b[0m 0.02021 \u001b[0m | \u001b[0m 0.9745  \u001b[0m | \u001b[0m 0.7287  \u001b[0m | \u001b[0m 5.46    \u001b[0m | \u001b[0m 191.5   \u001b[0m |\n",
      "| \u001b[0m 2       \u001b[0m | \u001b[0m 0.5034  \u001b[0m | \u001b[0m 0.08196 \u001b[0m | \u001b[0m 0.6456  \u001b[0m | \u001b[0m 0.4146  \u001b[0m | \u001b[0m 7.942   \u001b[0m | \u001b[0m 99.33   \u001b[0m |\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m 0.4946  \u001b[0m | \u001b[0m 0.03304 \u001b[0m | \u001b[0m 0.02401 \u001b[0m | \u001b[0m 0.09873 \u001b[0m | \u001b[0m 5.103   \u001b[0m | \u001b[0m 178.2   \u001b[0m |\n",
      "| \u001b[95m 4       \u001b[0m | \u001b[95m 0.5275  \u001b[0m | \u001b[95m 0.039   \u001b[0m | \u001b[95m 0.1855  \u001b[0m | \u001b[95m 0.9172  \u001b[0m | \u001b[95m 4.896   \u001b[0m | \u001b[95m 104.7   \u001b[0m |\n",
      "| \u001b[95m 5       \u001b[0m | \u001b[95m 0.5465  \u001b[0m | \u001b[95m 0.0959  \u001b[0m | \u001b[95m 0.1271  \u001b[0m | \u001b[95m 0.7473  \u001b[0m | \u001b[95m 3.037   \u001b[0m | \u001b[95m 221.4   \u001b[0m |\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.4971  \u001b[0m | \u001b[0m 0.07494 \u001b[0m | \u001b[0m 0.831   \u001b[0m | \u001b[0m 0.2529  \u001b[0m | \u001b[0m 6.918   \u001b[0m | \u001b[0m 77.88   \u001b[0m |\n",
      "| \u001b[95m 7       \u001b[0m | \u001b[95m 0.548   \u001b[0m | \u001b[95m 0.04988 \u001b[0m | \u001b[95m 0.3877  \u001b[0m | \u001b[95m 0.3779  \u001b[0m | \u001b[95m 3.126   \u001b[0m | \u001b[95m 209.4   \u001b[0m |\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 0.5061  \u001b[0m | \u001b[0m 0.07489 \u001b[0m | \u001b[0m 0.9712  \u001b[0m | \u001b[0m 0.9886  \u001b[0m | \u001b[0m 5.107   \u001b[0m | \u001b[0m 206.2   \u001b[0m |\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m 0.4892  \u001b[0m | \u001b[0m 0.02183 \u001b[0m | \u001b[0m 0.5781  \u001b[0m | \u001b[0m 0.8678  \u001b[0m | \u001b[0m 8.792   \u001b[0m | \u001b[0m 187.4   \u001b[0m |\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 0.5429  \u001b[0m | \u001b[0m 0.06734 \u001b[0m | \u001b[0m 0.4772  \u001b[0m | \u001b[0m 0.304   \u001b[0m | \u001b[0m 4.758   \u001b[0m | \u001b[0m 213.6   \u001b[0m |\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m 0.4984  \u001b[0m | \u001b[0m 0.09608 \u001b[0m | \u001b[0m 0.2417  \u001b[0m | \u001b[0m 0.2325  \u001b[0m | \u001b[0m 7.197   \u001b[0m | \u001b[0m 218.9   \u001b[0m |\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m 0.5108  \u001b[0m | \u001b[0m 0.06889 \u001b[0m | \u001b[0m 0.4963  \u001b[0m | \u001b[0m 0.7176  \u001b[0m | \u001b[0m 5.582   \u001b[0m | \u001b[0m 85.6    \u001b[0m |\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m 0.4587  \u001b[0m | \u001b[0m 0.0191  \u001b[0m | \u001b[0m 0.6856  \u001b[0m | \u001b[0m 0.1257  \u001b[0m | \u001b[0m 7.178   \u001b[0m | \u001b[0m 107.2   \u001b[0m |\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m 0.4945  \u001b[0m | \u001b[0m 0.01591 \u001b[0m | \u001b[0m 0.9046  \u001b[0m | \u001b[0m 0.1621  \u001b[0m | \u001b[0m 5.943   \u001b[0m | \u001b[0m 115.0   \u001b[0m |\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m 0.5348  \u001b[0m | \u001b[0m 0.05859 \u001b[0m | \u001b[0m 0.1979  \u001b[0m | \u001b[0m 0.7174  \u001b[0m | \u001b[0m 3.276   \u001b[0m | \u001b[0m 164.9   \u001b[0m |\n",
      "=====================================================================================\n",
      "L'optimization a mis : 4755 secondes.\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "m_clas_bo3.maximize(n_iter=10, init_points=5, acq='ucb')\n",
    "print(f\"L'optimization a mis : {time() - start:.0f} secondes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eta': 0.04988373441772504,\n",
       " 'gamma': 0.38774660406222805,\n",
       " 'learning_rate': 0.3779282680211212,\n",
       " 'max_depth': 3.125741502716014,\n",
       " 'n_estimators': 209.3536693510256}"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params_03 = m_clas_bo3.max['params']\n",
    "best_params_03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_03['max_depth']= int(best_params_03['max_depth'])\n",
    "best_params_03['n_estimators']= int(best_params_03['n_estimators'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model3 = xgb.XGBClassifier(objective = \"reg:logistic\", random_state = 47, **best_params_03)\n",
    "# création du modèle multilabels\n",
    "m_clas3 = BinaryRelevance(classifier = xgb_model3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L'entrainement a pris 437 secondes\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "m_clas3.fit(X_train_spacy_pca, y_train)\n",
    "print(f\"L'entrainement a pris {time() - start:.0f} secondes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5473448009607736"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_03 = m_clas3.predict(X_test_spacy_pca).toarray()\n",
    "f1_score(y_test, \n",
    "         pred_03, \n",
    "         average = \"weighted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OpClass",
   "language": "python",
   "name": "opclass"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
